#!/usr/bin/env python
import os
import pandas as pd
import numpy as np
import argparse
import scipy.stats as s
import h5py

from cnv_suite.simulate.cnv_profile import CNV_Profile
from capy import mut

def parse_args():
    parser = argparse.ArgumentParser(description = "generate requisite benchmarking files from simulated coverage profile")
    parser.add_argument("--sim_profile", required=True, help="path to cnv_suite coverage profile pickle")
    parser.add_argument("--output_dir", default=".", help="output directory path for file outputs")
    parser.add_argument("--purity", required = True, type=float, help="purity for simulated sample")
    parser.add_argument("--out_label", required = True, help="label for this simulated sample")
    parser.add_argument("--parallel", default=False, action='store_true', help="use multiple cores for generating files")
    subparsers = parser.add_subparsers(dest="command")
    
    ## hapaseg
    hapaseg_gen = subparsers.add_parser("hapaseg", help="generate hapaseg inputs")
    hapaseg_gen.add_argument("--normal_vcf_path", required=True, help="path to normal sample vcf file")
    hapaseg_gen.add_argument("--hetsite_depth_path", required=True, help="path to hetsite depth file")
    hapaseg_gen.add_argument("--covcollect_path", required=True, help="path to normal covcollect file")

    ## ascat
    ascat_gen = subparsers.add_parser("ascat", help="generate ascat inputs")
    ascat_gen.add_argument("--normal_vcf_path", required = True, help="path to normal sample vcf file")
    ascat_gen.add_argument("--variant_depth_path", required=True, help = "path to variant depth file")
    ascat_gen.add_argument("--filtered_variants_path", required=True, help="path to filtered variants file")
    
    ## facets
    facets_gen = subparsers.add_parser("facets", help="generate facets inputs")
    facets_gen.add_argument("--normal_vcf_path", required=True, help="path to normal sample vcf file")
    facets_gen.add_argument("--variant_depth_path", required=True, help="path to variant depth file")
    facets_gen.add_argument("--filtered_variants_path", required=True, help="path to filtered variants file")

    ## gatk
    gatk_gen = subparsers.add_parser("gatk", help="generate gatk inputs")
    gatk_gen.add_argument("--normal_vcf_path", required=True, help="path to normal sample vcf file")
    gatk_gen.add_argument("--variant_depth_path", required=True, help="path to variant depth file")
    gatk_gen.add_argument("--coverage_tsv_path", required=True, help="path to gatk coverage in covcorr format")
    gatk_gen.add_argument("--sim_normal_allelecounts_path", required=True, help="path to simulated normal allelecounts in gatk format")
    gatk_gen.add_argument("--raw_gatk_allelecounts_path", required=True, 
                            help="path to original gatk output allelecounts file from raw normal")
    gatk_gen.add_argument("--raw_gatk_coverage_path", required=True,
                            help="path to original gatk output coverage hdf5 from raw normal")
    
    ## hatchet
    hatchet_gen = subparsers.add_parser("hatchet", help="generate hatchet intermediate simulated files")
    hatchet_gen.add_argument("--normal_vcf_path", required=True, help="path to normal sample vcf file")
    hatchet_gen.add_argument("--tumor_baf_path", required=True, help="path to tumor baf generated by count-alleles")
    hatchet_gen.add_argument("--total_reads_paths", required=True, help="paths to <contig>.total.gz files generated by count-reads")
    hatchet_gen.add_argument("--thresholds_snps_paths", required=True, help="paths to <contig>.thresholds.gz files generated by count-reads")
    hatchet_gen.add_argument("--total_tsv_path", required=True, help="path to total.tsv file generated by count-reads")
    
    args = parser.parse_args()
    
    return args

def main():
    args = parse_args()
    if not os.path.isdir(args.output_dir):
        os.mkdir(args.output_dir)
    output_dir = os.path.realpath(args.output_dir)
    
    if args.purity < 0 or args.purity > 1:
        raise ValueError("purity must be in [0,1]")

    if args.command == "hapaseg":
        print("generating hapsaeg simulated files...")
        generate_hapaseg_files(sim_profile_pickle = args.sim_profile,
                               purity = args.purity,
                               normal_vcf_path= args.normal_vcf_path,
                               hetsite_depth_path = args.hetsite_depth_path,
                               covcollect_path = args.covcollect_path,
                               out_dir = output_dir,
                               out_label = args.out_label,
                               parallel = args.parallel)
    elif args.command == "ascat":
        print("generating ascat simulated files...")
        generate_ascat_files(sim_profile_pickle = args.sim_profile,
                             purity = args.purity,
                             normal_vcf_path = args.normal_vcf_path,
                             variant_depth_path = args.variant_depth_path,
                             filtered_variants_path = args.filtered_variants_path,
                             out_dir = output_dir,
                             out_label = args.out_label,
                             parallel = args.parallel)
    
    elif args.command == "facets":
        print("generating facets simulated files...")
        generate_facets_files(sim_profile_pickle = args.sim_profile,
                             purity = args.purity,
                             normal_vcf_path = args.normal_vcf_path,
                             variant_depth_path = args.variant_depth_path,
                             filtered_variants_path = args.filtered_variants_path,
                             out_dir = output_dir,
                             out_label = args.out_label,
                             parallel = args.parallel)

    elif args.command == "gatk":
        print("generating gatk simulated files...")        
        generate_gatk_files(sim_profile_pickle=args.sim_profile,
                        purity = args.purity,
                        normal_vcf_path=args.normal_vcf_path,
                        variant_depth_path=args.variant_depth_path,
                        raw_gatk_allelecounts_path=args.raw_gatk_allelecounts_path,
                        sim_normal_allelecounts_path=args.sim_normal_allelecounts_path,
                        coverage_tsv_path=args.coverage_tsv_path,
                        raw_gatk_coverage_path=args.raw_gatk_coverage_path,
                        out_dir=output_dir,
                        out_label=args.out_label,
                        parallel = args.parallel)
    elif args.command == "hatchet":
        print("generating hatchet simulated files...")
        generate_hatchet_files(sim_profile_pickle=args.sim_profile,
                        purity = args.purity,
                        normal_vcf_path=args.normal_vcf_path,
                        tumor_baf_path=args.tumor_baf_path,
                        total_reads_paths=args.total_reads_paths,
                        thresholds_snps_paths=args.thresholds_snps_paths,
                        total_tsv_path=args.total_tsv_path,
                        out_dir=output_dir,
                        out_label=args.out_label,
                        parallel = args.parallel)

    else:
        raise ValueError("Did not recognize command")

def convert_hapaseg_cov_to_gatk_hdf5(sim_dataframe, ref_hdf5, new_hdf5):
    f = h5py.File(ref_hdf5, 'r')
    cov_df = pd.read_csv(sim_dataframe, sep='\t')
    if len(cov_df) != f['counts/values'].shape[1]:
        raise ValueError("length of simulated coverage bin does not match normal ref file")
    new_f = h5py.File(new_hdf5, 'w')
    for k in f.keys():
        new_f.copy(f[k], k)
    new_f['counts/values'][:] = cov_df.covcorr.values.astype(float)
    new_f.close()
    f.close()


def generate_hapaseg_files(sim_profile_pickle = None,
                           purity = None,
                           normal_vcf_path = None,
                           hetsite_depth_path = None,
                           covcollect_path = None,
                           out_dir = None,
                           out_label = None,
                           parallel = False):
    
    default_profile=pd.read_pickle(sim_profile_pickle)
    
    # generate snvs
    snv_df, correct_phase_interval_trees = default_profile.generate_snvs(normal_vcf_path, hetsite_depth_path, purity, do_parallel=parallel)
    hets_df = snv_df.loc[(snv_df['NA12878'] != '1|1') &
                         (snv_df.REF.apply(lambda x : len(x)) == 1) & 
                         (snv_df.ALT.apply(lambda x : len(x)) == 1)]
    hets_df = hets_df.rename(columns={'CHROM': 'CONTIG',
                            'POS': 'POSITION',
                            'ref_count': 'REF_COUNT',
                            'alt_count': 'ALT_COUNT'})[['CONTIG', 'POSITION',
                                                      'REF_COUNT', 'ALT_COUNT']]
    hets_df.to_csv(os.path.join(out_dir, '{}_{}_hapaseg_hets.bed'.format(out_label, purity)), sep='\t', index=False)

    # generate coverage
    cov_df = default_profile.generate_coverage(purity, covcollect_path, do_parallel=parallel)
    cov_df = cov_df.rename(columns={'chrom': 'chr'})
    cov_df['chr'] = cov_df['chr'].apply(lambda x: 'chr' + str(x))
    cov_df.replace(to_replace={'chr':{'chr23':'chrX', 'chr24':'chrY'}}, inplace=True)
    cov_df[['chr', 'start', 'end',
            'covcorr', 'mean_fraglen',
            'sqrt_avg_fragvar', 'n_frags',
            'tot_reads', 'reads_flagged']].to_csv(os.path.join(out_dir, '{}_{}_hapaseg_coverage.bed'.format(out_label, purity)), sep='\t', index=False, header=False)

def generate_facets_files(sim_profile_pickle=None,
                          purity = None,
                          normal_vcf_path=None,
                          variant_depth_path=None,
                          filtered_variants_path=None,
                          out_dir=None,
                          out_label=None,
                          parallel=False):

    default_profile=pd.read_pickle(sim_profile_pickle)
    # generate snvs
    snv_df, correct_phase_interval_trees = default_profile.generate_snvs(normal_vcf_path, variant_depth_path, purity, do_parallel=parallel)
    snv_df = snv_df.loc[(snv_df.REF.apply(lambda x : len(x)) == 1) & (snv_df.ALT.apply(lambda x : len(x)) == 1)]

    snv_df = snv_df.rename({'CHROM':'chr', 'POS':'pos',
                            'ref_count':'File2R',
                            'alt_count':'File2A'}, axis=1)[['chr', 'pos',
                                                            'REF', 'ALT',
                                                            'File2R', 'File2A']]
    snv_df['chr'] = mut.convert_chr(snv_df['chr']).astype(int)

    # generate fake normal using binomial with N=30
    normal_df = pd.read_csv(filtered_variants_path, sep='\t')
    normal_df['File1A'] = s.binom.rvs(30, normal_df['t_altcount'] / normal_df['total_reads'])
    normal_df['File1R'] = 30 - normal_df['File1A']
    normal_df = normal_df.drop(['t_refcount', 't_altcount', 'total_reads'], axis=1)
    normal_df['chr'] = mut.convert_chr(normal_df['chr'])

    merged = snv_df.merge(normal_df, on=['chr', 'pos'], how='inner').drop_duplicates()
    merged.loc[:, ['File1E', 'File1D', 'File2E', 'File2D']] = 0
    merged = merged.rename({'chr':'Chromosome', 'pos': 'Position', 'REF':'Ref', 'ALT':'Alt'}, axis=1)
    merged = merged[['Chromosome', 'Position', 'Ref',
                     'Alt', 'File1R', 'File1A', 'File1E',
                     'File1D', 'File2R', 'File2A', 'File2E', 'File2D']]
    merged.to_csv(os.path.join(out_dir, '{}_{}_facets_input_counts.csv'.format(out_label, purity)), index=False)
        

# utility method for adding header back onto simulated allele count files
# inputs: og_tsv: allelecounts file with header intact (outputted by gatk CollectAlleleCounts)
#          sim_tsv: path to simulated allelecounts file witout header
def add_header_to_allelecounts(og_tsv, sim_tsv):
    header=[]
    with open(og_tsv, 'r') as f:
        fl = f.readlines()
        for l in fl:
            if l[0] == "@":
                header.append(l)
    header_str = ''.join(header)
    # gather tsv
    with open(sim_tsv, 'r') as f:
        sim_contents = f.read()
    # rewrite tsv
    with open(sim_tsv, 'w') as f:
        f.write(header_str)
        f.write(sim_contents)

# utility method for converting hapseg formatted coverage tsv into gatk fragcount hdf5 format
# inputs: sim_dataframe: path to coverage tsv
#         ref_hdf5: path to a hdf5 file generated by gatk with metadata intact
#                   and intervals/contigs matching the tsv
#         new_hdf5: path to create new hdf5 file at using covcorr from coverage tsv
#                   and metadata from ref_hdf5


def generate_gatk_files(sim_profile_pickle=None,
                        purity = None,
                        normal_vcf_path=None, # path to raw data vcf
                        variant_depth_path=None, # path to variant depths in og vcf
                        raw_gatk_allelecounts_path=None, # need original gatk allele counts output to copy its header
                        sim_normal_allelecounts_path=None, # path to simulated normal allelecounts file
                        coverage_tsv_path=None, # path to gatk coverage counts converted to hapaseg format
                        raw_gatk_coverage_path=None, # need original hdf5 file to copy metadata
                        out_dir=None,
                        out_label=None,
                        parallel=False):
                        
    default_profile = pd.read_pickle(sim_profile_pickle)   
    # generate snvs
    snv_df, correct_phase_interval_trees = default_profile.generate_snvs(normal_vcf_path, variant_depth_path, purity, do_parallel=parallel)
    # limit to SNPs
    snv_df = snv_df.loc[(snv_df.REF.apply(lambda x : len(x)) == 1) & (snv_df.ALT.apply(lambda x : len(x)) == 1)]
    # remove duplicates
    snv_df.loc[~snv_df.iloc[:, :2].duplicated(keep='first')]
    snv_df.CHROM = mut.convert_chr_back(snv_df.CHROM.astype(int))
    snv_df = snv_df.rename(columns={'CHROM': 'CONTIG',
                            'POS': 'POSITION',
                            'ref_count': 'REF_COUNT',
                            'alt_count': 'ALT_COUNT',
                            'REF':'REF_NUCLEOTIDE',
                            'ALT': 'ALT_NUCLEOTIDE'})
    sim_tumor_allelecount_out_path = os.path.join(out_dir, '{}_{}_gatk_allele.counts.tsv'.format(out_label, purity))
    snv_df[['CONTIG', 'POSITION', 'REF_COUNT', 'ALT_COUNT', 'REF_NUCLEOTIDE', 
            'ALT_NUCLEOTIDE']].to_csv(sim_tumor_allelecount_out_path, sep='\t', index=False)

    add_header_to_allelecounts(raw_gatk_allelecounts_path, sim_tumor_allelecount_out_path)

    # restrict normal counts to the tumor sites
    df = pd.read_csv(sim_normal_allelecounts_path, comment="@", sep='\t')
    df = df.merge(snv_df[['CONTIG', 'POSITION']], on = ['CONTIG', 'POSITION'], how='inner')

    norm_sim_allelecounts_out_path = os.path.join(out_dir, '{}_{}_gatk_sim_normal_allele.counts.tsv'.format(out_label, purity))
    df.to_csv(norm_sim_allelecounts_out_path, sep='\t', index=False)
 
    add_header_to_allelecounts(raw_gatk_allelecounts_path, norm_sim_allelecounts_out_path)

    # generate coverage
    sim_cov_out_path = os.path.join(out_dir, '{}_{}_gatk_sim_tumor_cov.tsv'.format(out_label, purity))
    default_profile.save_coverage_file(sim_cov_out_path, purity, coverage_tsv_path, do_parallel=parallel)

    # convert bed file into gatk hdf5 format
    convert_hapaseg_cov_to_gatk_hdf5(sim_cov_out_path, raw_gatk_coverage_path, 
                                     os.path.join(out_dir, '{}_{}_gatk_sim_tumor.frag.counts.hdf5'.format(out_label, purity)))


def generate_ascat_files(sim_profile_pickle=None,
                         purity = None,
                         normal_vcf_path=None,
                         variant_depth_path=None,
                         filtered_variants_path=None,
                         out_dir = None,
                         out_label=None,
                         parallel=False):
    
    default_profile = pd.read_pickle(sim_profile_pickle) 
     
    # generate snvs
    snv_df, correct_phase_interval_trees = default_profile.generate_snvs(normal_vcf_path, variant_depth_path, purity, do_parallel=parallel)
    snv_df = snv_df.loc[(snv_df.REF.apply(lambda x : len(x)) == 1) & (snv_df.ALT.apply(lambda x : len(x)) == 1)]

    snv_df = snv_df.rename({'CHROM':'chr', 'POS':'pos',
                            'ref_count':'t_refcount',
                            'alt_count':'t_altcount'}, axis=1)[['chr', 'pos',
                                                                'REF', 'ALT',
                                                                 't_refcount', 't_altcount',
                                                                 'adjusted_depth']]
    snv_df['chr'] = mut.convert_chr(snv_df['chr']).astype(int)
    snv_df = snv_df.loc[snv_df.adjusted_depth > 0]

    # generate fake normal using binomial with N=30
    normal_df = pd.read_csv(filtered_variants_path, sep='\t')
    normal_df['n_altcount'] = s.binom.rvs(30, normal_df['t_altcount'] / normal_df['total_reads'])
    normal_df['n_refcount'] = 30 - normal_df['n_altcount']
    normal_df = normal_df.drop(['t_refcount', 't_altcount'], axis=1)
    normal_df['chr'] = mut.convert_chr(normal_df['chr'])

    merged = snv_df.merge(normal_df, on=['chr', 'pos'], how='inner').drop_duplicates()
    # calculate ascat input logR and BAF
    merged['tumorLogR'] = merged['adjusted_depth'] / merged['total_reads']
    merged['tumorLogR'] = np.log2(merged['tumorLogR'] / merged['tumorLogR'].mean())
    # ascat passes around a normalLogR file but never actually defines these values (nor are they ever used)
    # we will make a dummy file 
    merged['normalLogR'] = 0

    merged['normalBAF'] = np.nan
    merged['tumorBAF'] = np.nan
    # for unexplained reasons ascat also randomizes A and B alleles
    mask = np.random.rand(len(merged)) > 0.5
    merged.loc[mask, 'normalBAF'] = merged.loc[mask, 'n_refcount'] / merged.loc[mask, 'total_reads']
    merged.loc[~mask, 'normalBAF'] = merged.loc[~mask, 'n_altcount'] / merged.loc[~mask, 'total_reads']
    merged.loc[mask, 'tumorBAF'] = merged.loc[mask, 't_refcount'] / merged.loc[mask, 'adjusted_depth']
    merged.loc[~mask, 'tumorBAF'] = merged.loc[~mask, 't_altcount'] / merged.loc[~mask, 'adjusted_depth']

    # switch back to string X
    merged['chr'] = merged['chr'].apply(lambda x: 'X' if x ==23 else str(x))
    merged = merged.rename({'chr':'chrs'}, axis=1)
    # ascat expects the index to be in chr_pos form
    merged = merged.set_index(merged.apply(lambda x: x.chrs + '_' + str(x.pos), axis = 1))
    
    sample_name = '{}_{}_ascat'.format(out_label, purity)
    # ascat returns 4 seperate files each with one column of data
    merged.rename({'tumorLogR':sample_name}, axis=1)[['chrs', 'pos', sample_name]].to_csv(os.path.join(out_dir, '{}_{}_ascat_tumor_LogR.txt'.format(out_label, purity)), sep='\t')
    merged.rename({'normalLogR':sample_name}, axis=1)[['chrs', 'pos', sample_name]].to_csv(os.path.join(out_dir, '{}_{}_ascat_normal_LogR.txt'.format(out_label, purity)), sep='\t')
    merged.rename({'tumorBAF':sample_name}, axis=1)[['chrs', 'pos', sample_name]].to_csv(os.path.join(out_dir, '{}_{}_ascat_tumor_BAF.txt'.format(out_label, purity)), sep='\t')
    merged.rename({'normalBAF':sample_name}, axis=1)[['chrs', 'pos', sample_name]].to_csv(os.path.join(out_dir, '{}_{}_ascat_normal_BAF.txt'.format(out_label, purity)), sep='\t')

def generate_hatchet_files(sim_profile_pickle=None,
                           purity = None,
                           normal_vcf_path=None,
                           tumor_baf_path=None,
                           total_reads_paths=None,
                           thresholds_snps_paths=None,
                           total_tsv_path=None,
                           out_dir = "./",
                           out_label=None,
                           parallel=False):
    
    # save samples.txt file to same output_dir (combine-counts expects it to be in same directory as thresholds and total.gz files
    input_samples_txt = os.path.join(os.split(total_reads_paths[0])[0], "samples.txt")
    output_samples_txt = os.path.join(out_dir, "rdr", "samples.txt")
    os.subprocess(f'cp {input_samples_txt} {output_samples_txt}') 
    
    default_profile = pd.read_pickle(sim_profile_pickle)   
    
    total_fn_list_contigs = []
    for total_fn, threshold_fn in zip(total_reads_paths, thresholds_snps_paths):
        total_fn_list_contigs.append(os.path.split(total_fn)[1])
        this_read_df = pd.read_csv(total_fn, sep=' ', header=None, names=['n_int_reads', 'n_pos_reads', 't_int_reads', 't_pos_reads'])
        this_thresholds_df = pd.read_csv(threshold_fn, sep='\t', header=None, names=['threshold'])
        
        # copy thresholds files to output directory (stored alongside total counts files)
        os.subprocess(f'cp {threshold_fn} {os.path.join(out_dir, "rdr", os.path.split(threshold_fn)[-1])}')

        int_corr = pd.concat([this_thresholds_df['threshold'], pd.Series(np.append(this_thresholds_df.loc[1:, 'threshold'], this_thresholds_df.values[-1])), this_read_df['t_int_reads']], axis=1)
        int_corr.columns = ['start', 'end', 'covcorr']
        int_corr['contig'] = chr
        pos_corr = pd.concat([this_thresholds_df['threshold'], this_thresholds_df['threshold'] + 1, this_read_df['t_pos_reads']], axis=1)
        pos_corr.columns = ['start', 'end', 'covcorr']
        pos_corr['contig'] = chr

        this_read_df['contig'] = chr
        interval_corr_df.append(int_corr)
        position_corr_df.append(pos_corr)
        read_combined_df.append(this_read_df)

    interval_corr_df = pd.concat(interval_corr_df)
    position_corr_df = pd.concat(position_corr_df)
    read_combined_df = pd.concat(read_combined_df)
    
    #
    int_counts_sim_fn = os.path.join(out_dir, 'interval_counts.for_simulation_input.txt')
    pos_counts_sim_fn = os.path.join(out_dir, 'position_counts.for_simulation_input.txt')
    snp_counts_sim_fn = os.path.join(out_dir, 'snp_counts.for_simulation_input.txt')

    interval_corr_df.to_csv(int_counts_sim_fn, sep='\t', index=False, columns=['contig', 'start', 'end', 'covcorr'], header=False)
    position_corr_df.to_csv(pos_counts_sim_fn, sep='\t', index=False, columns=['contig', 'start', 'end', 'covcorr'], header=False)

    # easy to do with bash also (just delete sample column and add header)
    snp_counts_1bed = pd.read_csv(tumor_baf_path, sep='\t', header=None, names=['contig', 'pos', 'sample', 'ref_count', 'alt_count'])
    snp_counts_1bed.to_csv(snp_counts_sim_fn, sep='\t', index=False, columns=['contig', 'pos', 'ref_count', 'alt_count'])
    
    int_counts_tr_fn = os.path.join(out_dir, 'interval_counts.transformed.txt')
    pos_counts_tr_fn = os.path.join(out_dir, 'position_counts.transformed.txt')
    snp_counts_tr_fn = os.path.join(out_dir, 'snp_counts.transformed.txt')

    # alter total counts based on profile, for intervals (between SNP thresholds) and at SNP thresholds
    default_profile.save_coverage_file(int_counts_tr_fn, purity, int_counts_sim_fn)
    default_profile.save_coverage_file(pos_counts_tr_fn, purity, pos_counts_sim_fn)
    # alter SNP counts based on profile in tumor.1bed
    default_profile.save_hets_file(snp_counts_tr_fn, normal_vcf_path, snp_counts_sim_fn, purity, ref_alt=True)
    
    # Read back in
    # Read Depths
    interval_corr_df_transformed = pd.read_csv(int_counts_tr_fn, sep='\t', header=0, usecols=['chr', 'start', 'end', 'covcorr'])
    position_corr_df_transformed = pd.read_csv(pos_counts_tr_fn, sep='\t', header=0, usecols=['chr', 'start', 'end', 'covcorr'])

    read_combined_df['t_int_reads'] = interval_corr_df_transformed['covcorr']
    read_combined_df['t_pos_reads'] = position_corr_df_transformed['covcorr']
    
    # Adjust Normal read counts to be 30x coverage - can remove this if we want to use true normal values (have separate bam for tumor and normal)
    # each bin has (bin size) * 30x coverage / average_read_length
    read_combined_df['n_int_reads'] = (interval_corr_df['end'] - interval_corr_df['start']) * 30 / 150
    # every position has 30x coverage
    read_combined_df['n_pos_reads'] = 30

    for total_fn in total_fn_list_contigs:
        read_combined_df[read_combined_df['contig'] == total_fn.split('.')[0]].drop(columns='contig').to_csv(os.path.join(out_dir, f'rdr/{total_fn}'), sep='\t', header=False, index=False, compression='gzip')

    # Total Counts
    total_counts = pd.read_csv(total_tsv_path, sep='\t', header=None)
    total_counts.iloc[0,1] = read_combined_df['n_int_reads'].sum()
    total_counts.iloc[1,1] = read_combined_df['t_int_reads'].sum()
    total_counts.to_csv(os.path.join(out_dir, 'rdr/total.tsv'), sep='\t', header=False, index=False)
    
    # SNP Counts
    snp_df_transformed = pd.read_csv(snp_counts_tr_fn, sep='\t', header=0, names=['contig', 'pos', 'ref_count', 'alt_count'])
    snp_counts_1bed['ref_count'] = snp_df_transformed['ref_count']
    snp_counts_1bed['alt_count'] = snp_df_transformed['alt_count']
    snp_counts_1bed.to_csv(os.path.join(out_dir, 'baf/tumor.1bed'), sep='\t', header=False, index=False)
            
if __name__ == "__main__":
    main()
