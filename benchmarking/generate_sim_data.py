#!/usr/bin/env python
import os
import pandas as pd
import numpy as np
import argparse
import scipy.stats as s
import h5py
import shutil
import glob

from preprocess_raw_data import load_callstats
from cnv_suite.simulate.cnv_profile import CNV_Profile
from capy import mut

def parse_args():
    parser = argparse.ArgumentParser(description = "generate requisite benchmarking files from simulated coverage profile")
    parser.add_argument("--sim_profile", required=True, help="path to cnv_suite coverage profile pickle")
    parser.add_argument("--output_dir", default=".", help="output directory path for file outputs")
    parser.add_argument("--purity", required = True, type=float, help="purity for simulated sample")
    parser.add_argument("--out_label", required = True, help="label for this simulated sample")
    parser.add_argument("--parallel", default=False, action='store_true', help="use multiple cores for generating files")
    subparsers = parser.add_subparsers(dest="command")
    
    ## hapaseg
    hapaseg_gen = subparsers.add_parser("hapaseg", help="generate hapaseg inputs")
    hapaseg_gen.add_argument("--normal_vcf_path", required=True, help="path to normal sample vcf file")
    hapaseg_gen.add_argument("--hetsite_depth_path", required=True, help="path to hetsite depth file")
    hapaseg_gen.add_argument("--covcollect_path", required=True, help="path to normal covcollect file")

    ## ascat
    ascat_gen = subparsers.add_parser("ascat", help="generate ascat inputs")
    ascat_gen.add_argument("--normal_vcf_path", required = True, help="path to normal sample vcf file")
    ascat_gen.add_argument("--variant_depth_path", required=True, help = "path to variant depth file")
    ascat_gen.add_argument("--filtered_variants_path", required=False, help="path to filtered variants file to generate fake allelecounts")
    ascat_gen.add_argument("--normal_callstats_path", required=False, help="path to normal callstats file to use real allelecounts")
    ascat_gen.add_argument("--unmatched_normal_callstats", required=False, default=True,
                               dest='matched_normal_callstats', action='store_false',
                               help="flag for indicating normal callstats file is not matched to the normal vcf and hence normal het counts should be simulated"
                            )
    
    ## facets
    facets_gen = subparsers.add_parser("facets", help="generate facets inputs")
    facets_gen.add_argument("--normal_vcf_path", required=True, help="path to normal sample vcf file")
    facets_gen.add_argument("--variant_depth_path", required=False, help="path to callstats derived variant depth file")
    facets_gen.add_argument("--facets_allelecounts_path", required=False, help="path to facets allelecounts file")
    facets_gen.add_argument("--filtered_variants_path", required=False, help="path to filtered variants file to generate fake allelecounts")
    facets_gen.add_argument("--normal_callstats_path", required=False, help="path to normal callstats file to use real allelecounts")
    facets_gen.add_argument("--unmatched_normal_callstats", required=False, default=True,
                               dest='matched_normal_callstats', action='store_false',
                               help="flag for indicating normal callstats file is not matched to the normal vcf and hence normal het counts should be simulated"
                            )
    ## gatk
    gatk_gen = subparsers.add_parser("gatk", help="generate gatk inputs")
    gatk_gen.add_argument("--normal_vcf_path", required=True, help="path to normal sample vcf file")
    gatk_gen.add_argument("--variant_depth_path", required=True, help="path to variant depth file")
    gatk_gen.add_argument("--coverage_tsv_path", required=True, help="path to gatk coverage in covcorr format")
    gatk_gen.add_argument("--sim_normal_allelecounts_path", required=True, help="path to simulated normal allelecounts in gatk format")
    gatk_gen.add_argument("--raw_gatk_allelecounts_path", required=True, 
                            help="path to original gatk output allelecounts file from raw normal")
    gatk_gen.add_argument("--raw_gatk_coverage_path", required=True,
                            help="path to original gatk output coverage hdf5 from raw normal")
    
    ## hatchet
    hatchet_gen = subparsers.add_parser("hatchet", help="generate hatchet intermediate simulated files")
    hatchet_gen.add_argument("--normal_vcf_path", required=True, help="path to normal sample vcf file")
    hatchet_gen.add_argument("--tumor_baf_path", required=True, help="path to tumor baf generated by count-alleles")
    hatchet_gen.add_argument("--pos_counts_file", required=True, help="path to aggregated positional counts from preprocessing")
    hatchet_gen.add_argument("--int_counts_file", required=True, help="path to aggregated interval counts from preprocessing")
    hatchet_gen.add_argument("--snp_counts_file", required=True, help="path to aggregated snp counts from preprocessing")
    hatchet_gen.add_argument("--read_combined_file", required=True, help="path to aggregated counts file from preprocessing")
    args = parser.parse_args()
    
    return args

def main():
    args = parse_args()
    if not os.path.isdir(args.output_dir):
        os.mkdir(args.output_dir)
    output_dir = os.path.realpath(args.output_dir)
    
    if args.purity < 0 or args.purity > 1:
        raise ValueError("purity must be in [0,1]")

    if args.command == "hapaseg":
        print("generating hapsaeg simulated files...")
        generate_hapaseg_files(sim_profile_pickle = args.sim_profile,
                               purity = args.purity,
                               normal_vcf_path= args.normal_vcf_path,
                               hetsite_depth_path = args.hetsite_depth_path,
                               covcollect_path = args.covcollect_path,
                               out_dir = output_dir,
                               out_label = args.out_label,
                               parallel = args.parallel)

    elif args.command == "ascat":
        print("generating ascat simulated files...")
        generate_ascat_files(sim_profile_pickle = args.sim_profile,
                             purity = args.purity,
                             normal_vcf_path = args.normal_vcf_path,
                             variant_depth_path = args.variant_depth_path,
                             filtered_variants_path = args.filtered_variants_path,
                             normal_callstats_path = args.normal_callstats_path,
                             matched_normal_callstats = args.matched_normal_callstats,
                             out_dir = output_dir,
                             out_label = args.out_label,
                             parallel = args.parallel)
    
    elif args.command == "facets":
        print("generating facets simulated files...")
        generate_facets_files(sim_profile_pickle = args.sim_profile,
                             purity = args.purity,
                             normal_vcf_path = args.normal_vcf_path,
                             variant_depth_path = args.variant_depth_path,
                             facets_allelecounts_path = args.facets_allelecounts_path,
                             filtered_variants_path = args.filtered_variants_path,
                             normal_callstats_path = args.normal_callstats_path,
                             matched_normal_callstats = args.matched_normal_callstats,
                             out_dir = output_dir,
                             out_label = args.out_label,
                             parallel = args.parallel)

    elif args.command == "gatk":
        print("generating gatk simulated files...")        
        generate_gatk_files(sim_profile_pickle=args.sim_profile,
                        purity = args.purity,
                        normal_vcf_path=args.normal_vcf_path,
                        variant_depth_path=args.variant_depth_path,
                        raw_gatk_allelecounts_path=args.raw_gatk_allelecounts_path,
                        sim_normal_allelecounts_path=args.sim_normal_allelecounts_path,
                        coverage_tsv_path=args.coverage_tsv_path,
                        raw_gatk_coverage_path=args.raw_gatk_coverage_path,
                        out_dir=output_dir,
                        out_label=args.out_label,
                        parallel = args.parallel)

    elif args.command == "hatchet":
        print("generating hatchet simulated files...")
        generate_hatchet_files(sim_profile_pickle=args.sim_profile,
                        purity = args.purity,
                        normal_vcf_path=args.normal_vcf_path,
                        tumor_baf_path=args.tumor_baf_path,
                        int_counts_sim_fn = args.int_counts_file, # files from preprocessing
                        pos_counts_sim_fn = args.pos_counts_file,
                        snp_counts_sim_fn = args.snp_counts_file,
                        read_combined_fn = args.read_combined_file,
                        out_dir=output_dir,
                        out_label=args.out_label,
                        parallel = args.parallel)

    else:
        raise ValueError("Did not recognize command")

def convert_hapaseg_cov_to_gatk_hdf5(sim_dataframe, ref_hdf5, new_hdf5):
    f = h5py.File(ref_hdf5, 'r')
    cov_df = pd.read_csv(sim_dataframe, sep='\t')
    if len(cov_df) != f['counts/values'].shape[1]:
        raise ValueError("length of simulated coverage bin does not match normal ref file")
    new_f = h5py.File(new_hdf5, 'w')
    for k in f.keys():
        new_f.copy(f[k], k)
    new_f['counts/values'][:] = cov_df.covcorr.values.astype(float)
    new_f.close()
    f.close()


def generate_hapaseg_files(sim_profile_pickle = None,
                           purity = None,
                           normal_vcf_path = None,
                           hetsite_depth_path = None,
                           covcollect_path = None,
                           out_dir = None,
                           out_label = None,
                           parallel = False):
    
    default_profile=pd.read_pickle(sim_profile_pickle)
    
    # generate snvs
    snv_df, correct_phase_interval_trees = default_profile.generate_snvs(normal_vcf_path, hetsite_depth_path, purity, do_parallel=parallel)
    hets_df = snv_df.loc[(snv_df['NA12878'] != '1|1') &
                         (snv_df.REF.apply(lambda x : len(x)) == 1) & 
                         (snv_df.ALT.apply(lambda x : len(x)) == 1)]
    hets_df = hets_df.rename(columns={'CHROM': 'CONTIG',
                            'POS': 'POSITION',
                            'ref_count': 'REF_COUNT',
                            'alt_count': 'ALT_COUNT'})[['CONTIG', 'POSITION',
                                                      'REF_COUNT', 'ALT_COUNT']]
    hets_df.to_csv(os.path.join(out_dir, '{}_{}_hapaseg_hets.bed'.format(out_label, purity)), sep='\t', index=False)

    # generate coverage
    cov_df = default_profile.generate_coverage(purity, covcollect_path, do_parallel=parallel)
    cov_df = cov_df.rename(columns={'chrom': 'chr'})
    cov_df['chr'] = cov_df['chr'].apply(lambda x: 'chr' + str(x))
    cov_df.replace(to_replace={'chr':{'chr23':'chrX', 'chr24':'chrY'}}, inplace=True)
    cov_df[['chr', 'start', 'end',
            'covcorr', 'mean_fraglen',
            'sqrt_avg_fragvar', 'n_frags',
            'tot_reads', 'reads_flagged']].to_csv(os.path.join(out_dir, '{}_{}_hapaseg_coverage.bed'.format(out_label, purity)), sep='\t', index=False, header=False)

"""
generate simulated facets files

user can pass either a variants depth path derived from mutect1 callstats file
or a facets allecounts file to use as counts for the simulation. When passing in a variants
depth path from mutect, the normal counts can either be derived from a sepearte normal sample
by passing a normal callstats file (unfiltered) or by passing a filtered version of the 
tumor callstats file, which will be used to generate uniform 30x coverage fake normal counts. 
if a seperate normal sample is passed which does not match the normal vcf, make sure to 
set mathed_normal_callstats to False to ensure that normal allele counts are generated using 
a binomial with N= depth at SNP site in seperate normal and p=.5
"""
def generate_facets_files(sim_profile_pickle=None,
                          purity = None,
                          normal_vcf_path=None,
                          variant_depth_path=None,
                          facets_allelecounts_path=None,
                          filtered_variants_path=None,
                          normal_callstats_path = None,
                          matched_normal_callstats=True,
                          out_dir=None,
                          out_label=None,
                          parallel=False):

    if not os.path.isdir(out_dir):
        os.path.mkdir(out_dir)

    generated_depths_file = False
    
    if variant_depth_path is None:
        # we will try to use the facets allelecounts instead
        if facets_allelecounts_path is None:
            raise ValueError("need to pass either a variant depth path or facets allelecounts")
        facets_df = pd.read_csv(facets_allelecounts_path)
        facets_df['depth'] = facets_df['File2A'] + facets_df['File2R']
        variant_depth_path = os.path.join(out_dir, 'facets_var_depths.tsv')
        facets_df[['Chromosome', 'Position', 'depth']].to_csv(variant_depth_path, header=False, sep='\t', index=False)
        generated_depths_file = True
    
    default_profile=pd.read_pickle(sim_profile_pickle)
    # generate snvs
    snv_df, correct_phase_interval_trees = default_profile.generate_snvs(normal_vcf_path, variant_depth_path, purity, do_parallel=parallel)
    snv_df = snv_df.loc[(snv_df.REF.apply(lambda x : len(x)) == 1) & (snv_df.ALT.apply(lambda x : len(x)) == 1)]
    
    if generated_depths_file:
        # remove this now useless file to save space
        os.remove(variant_depth_path) 
   
    if facets_allelecounts_path is not None:
        # use facets allelecounts for normal
        snv_df = snv_df.rename({'CHROM':'Chromosome', 'POS':'Position',
                                'ref_count':'File2R',
                                'alt_count':'File2A'}, axis=1)[['Chromosome', 'Position',
                                                                'File2R', 'File2A']]
        facets_df = facets_df.drop(['File2R', 'File2A'], axis=1)
        snv_df = snv_df[['Chromosome', 'Position', 'File2A', 'File2R']]
        snv_df.loc[:, 'Chromosome'] = mut.convert_chr_back(snv_df['Chromosome'].astype(int))
        facets_df = facets_df.merge(snv_df, on = ['Chromosome', 'Position'], how = 'left')
        del snv_df
         
        # the additional loci that facets adds for coverage probes will be thrown out by the snp
        # simulator. simulate these seperately with coverage gen
        cov_df = facets_df.loc[facets_df['File2A'].isnull()]
        cov_df = cov_df.loc[:, ['Chromosome', 'Position', 'depth']]
        cov_df['end'] = cov_df['Position'] + 1
        cov_df = cov_df.rename({'Chromosome':'chrom', 'Position':'start', 'depth':'covcorr'}, axis = 1)

        out_df = default_profile.compute_coverage(cov_df, 0.7, do_parallel = parallel)
        out_df['chrom'] = mut.convert_chr_back(out_df['chrom'].astype(int))
        out_df = out_df.drop(['end', 'ploidy', 'covcorr_original'], axis=1)
        out_df = out_df.rename({'chrom':'Chromosome', 'start':'Position'}, axis=1)
        facets_df = facets_df.merge(out_df, on = ['Chromosome', 'Position'], how = 'left')
        facets_df.loc[facets_df['File2A'].isnull(), 'File2R'] = facets_df.loc[facets_df['File2A'].isnull(), 'covcorr']
        facets_df.loc[facets_df['File2A'].isnull(), 'File2A'] = 0
        facets_df['File2A'] = facets_df['File2A'].astype(int)
    
        facets_df = facets_df[['Chromosome', 'Position', 'Ref',
                         'Alt', 'File1R', 'File1A', 'File1E',
                         'File1D', 'File2R', 'File2A', 'File2E', 'File2D']]
        facets_df.to_csv(os.path.join(out_dir, '{}_{}_facets_input_counts.csv.gz'.format(out_label, purity)), index=False)
        
    else:
        snv_df = snv_df.rename({'CHROM':'chr', 'POS':'pos',
                                'ref_count':'File2R',
                                'alt_count':'File2A'}, axis=1)[['chr', 'pos',
                                                                'REF', 'ALT',
                                                                'File2R', 'File2A']]
        snv_df['chr'] = mut.convert_chr(snv_df['chr']).astype(int)

        
        if normal_callstats_path is not None:
            # use allele counts from normal
            normal_df = load_callstats(normal_callstats_path)
            normal_df = normal_df[['chr', 'pos', 't_refcount', 't_altcount']]
            if matched_normal_callstats:
                # the tumor callstats at the vcf variant sites will correspond
                #  to hets so we use them directly
                normal_df = normal_df.rename({'t_refcount': 'File1R', 't_altcount':'File1A'}, axis=1)
            else:
                # the tumor callstats counts are likely not hets since this normal is unmatched
                # we must generate het site counts to allow facets to use these sites
                normal_df['depth'] = normal_df['t_refcount'] + normal_df['t_altcount']
                normal_df['new_ref'] = s.binom.rvs(normal_df['depth'], 0.5)
                normal_df['new_alt'] = normal_df['depth'] - normal_df['new_ref']
                normal_df = normal_df.rename({'new_ref': 'File1R', 'new_alt':'File1A'}, axis=1)
                normal_df = normal_df.drop(columns = ['depth', 't_refcount', 't_altcount'])
            normal_df['chr'] = mut.convert_chr(normal_df['chr'])
        
        elif filtered_variants_path is not None:
            # generate fake normal using binomial with N=30
            normal_df = pd.read_csv(filtered_variants_path, sep='\t')
            normal_df['File1A'] = s.binom.rvs(30, normal_df['t_altcount'] / normal_df['total_reads'])
            normal_df['File1R'] = 30 - normal_df['File1A']
            normal_df = normal_df.drop(['t_refcount', 't_altcount', 'total_reads'], axis=1)
            normal_df['chr'] = mut.convert_chr(normal_df['chr'])

        else:
            raise ValueError("normal callstats or filtered variants file must be passed")

        merged = snv_df.merge(normal_df, on=['chr', 'pos'], how='inner').drop_duplicates()
        merged.loc[:, ['File1E', 'File1D', 'File2E', 'File2D']] = 0
        merged = merged.rename({'chr':'Chromosome', 'pos': 'Position', 'REF':'Ref', 'ALT':'Alt'}, axis=1)
        merged = merged[['Chromosome', 'Position', 'Ref',
                         'Alt', 'File1R', 'File1A', 'File1E',
                         'File1D', 'File2R', 'File2A', 'File2E', 'File2D']]
        merged.to_csv(os.path.join(out_dir, '{}_{}_facets_input_counts.csv'.format(out_label, purity)), index=False)
    
# utility method for adding header back onto simulated allele count files
# inputs: og_tsv: allelecounts file with header intact (outputted by gatk CollectAlleleCounts)
#          sim_tsv: path to simulated allelecounts file witout header
def add_header_to_allelecounts(og_tsv, sim_tsv):
    header=[]
    with open(og_tsv, 'r') as f:
        fl = f.readlines()
        for l in fl:
            if l[0] == "@":
                header.append(l)
    header_str = ''.join(header)
    # gather tsv
    with open(sim_tsv, 'r') as f:
        sim_contents = f.read()
    # rewrite tsv
    with open(sim_tsv, 'w') as f:
        f.write(header_str)
        f.write(sim_contents)

# utility method for converting hapseg formatted coverage tsv into gatk fragcount hdf5 format
# inputs: sim_dataframe: path to coverage tsv
#         ref_hdf5: path to a hdf5 file generated by gatk with metadata intact
#                   and intervals/contigs matching the tsv
#         new_hdf5: path to create new hdf5 file at using covcorr from coverage tsv
#                   and metadata from ref_hdf5


def generate_gatk_files(sim_profile_pickle=None,
                        purity = None,
                        normal_vcf_path=None, # path to raw data vcf
                        variant_depth_path=None, # path to variant depths in og vcf
                        raw_gatk_allelecounts_path=None, # need original gatk allele counts output to copy its header
                        sim_normal_allelecounts_path=None, # path to simulated normal allelecounts file
                        coverage_tsv_path=None, # path to gatk coverage counts converted to hapaseg format
                        raw_gatk_coverage_path=None, # need original hdf5 file to copy metadata
                        out_dir=None,
                        out_label=None,
                        parallel=False):
                        
    default_profile = pd.read_pickle(sim_profile_pickle)   
    # generate snvs
    snv_df, correct_phase_interval_trees = default_profile.generate_snvs(normal_vcf_path, variant_depth_path, purity, do_parallel=parallel)
    # limit to SNPs
    snv_df = snv_df.loc[(snv_df.REF.apply(lambda x : len(x)) == 1) & (snv_df.ALT.apply(lambda x : len(x)) == 1)]
    # remove duplicates
    snv_df.loc[~snv_df.iloc[:, :2].duplicated(keep='first')]
    snv_df.CHROM = mut.convert_chr_back(snv_df.CHROM.astype(int))
    snv_df = snv_df.rename(columns={'CHROM': 'CONTIG',
                            'POS': 'POSITION',
                            'ref_count': 'REF_COUNT',
                            'alt_count': 'ALT_COUNT',
                            'REF':'REF_NUCLEOTIDE',
                            'ALT': 'ALT_NUCLEOTIDE'})
    sim_tumor_allelecount_out_path = os.path.join(out_dir, '{}_{}_gatk_allele.counts.tsv'.format(out_label, purity))
    snv_df[['CONTIG', 'POSITION', 'REF_COUNT', 'ALT_COUNT', 'REF_NUCLEOTIDE', 
            'ALT_NUCLEOTIDE']].to_csv(sim_tumor_allelecount_out_path, sep='\t', index=False)

    add_header_to_allelecounts(raw_gatk_allelecounts_path, sim_tumor_allelecount_out_path)

    # restrict normal counts to the tumor sites
    df = pd.read_csv(sim_normal_allelecounts_path, comment="@", sep='\t')
    df = df.merge(snv_df[['CONTIG', 'POSITION']], on = ['CONTIG', 'POSITION'], how='inner')

    norm_sim_allelecounts_out_path = os.path.join(out_dir, '{}_{}_gatk_sim_normal_allele.counts.tsv'.format(out_label, purity))
    df.to_csv(norm_sim_allelecounts_out_path, sep='\t', index=False)
 
    add_header_to_allelecounts(raw_gatk_allelecounts_path, norm_sim_allelecounts_out_path)

    # generate coverage
    sim_cov_out_path = os.path.join(out_dir, '{}_{}_gatk_sim_tumor_cov.tsv'.format(out_label, purity))
    default_profile.save_coverage_file(sim_cov_out_path, purity, coverage_tsv_path, do_parallel=parallel)

    # convert bed file into gatk hdf5 format
    convert_hapaseg_cov_to_gatk_hdf5(sim_cov_out_path, raw_gatk_coverage_path, 
                                     os.path.join(out_dir, '{}_{}_gatk_sim_tumor.frag.counts.hdf5'.format(out_label, purity)))


def generate_ascat_files(sim_profile_pickle=None,
                         purity = None,
                         normal_vcf_path=None,
                         variant_depth_path=None,
                         filtered_variants_path=None,
                         normal_callstats_path=None,
                         matched_normal_callstats=True, # set to false if normal vcf does not match normal callstats
                         out_dir = None,
                         out_label=None,
                         parallel=False):
    
    default_profile = pd.read_pickle(sim_profile_pickle) 
     
    # generate snvs
    snv_df, correct_phase_interval_trees = default_profile.generate_snvs(normal_vcf_path, variant_depth_path, purity, do_parallel=parallel)
    snv_df = snv_df.loc[(snv_df.REF.apply(lambda x : len(x)) == 1) & (snv_df.ALT.apply(lambda x : len(x)) == 1)]

    snv_df = snv_df.rename({'CHROM':'chr', 'POS':'pos',
                            'ref_count':'t_refcount',
                            'alt_count':'t_altcount'}, axis=1)[['chr', 'pos',
                                                                'REF', 'ALT',
                                                                 't_refcount', 't_altcount',
                                                                 'adjusted_depth']]
    snv_df['chr'] = mut.convert_chr(snv_df['chr']).astype(int)
    snv_df = snv_df.loc[snv_df.adjusted_depth > 1] # ascat uses a min of two count in tumor

    if normal_callstats_path is not None:
        # use allele counts from normal
        normal_df = load_callstats(normal_callstats_path)
        normal_df = normal_df[['chr', 'pos', 't_refcount', 't_altcount']]
        if matched_normal_callstats:
            # t ref and alt counts match the vcf and hence should be het sites that we can use directly
            normal_df = normal_df.rename({'t_refcount': 'n_refcount', 't_altcount':'n_altcount'}, axis=1)
        else:
            # counts at vcf sites are not hets and hence we need to generate het looking counts using binomial
            normal_df['depth'] = normal_df['t_refcount'] + normal_df['t_altcount']
            normal_df['new_ref'] = s.binom.rvs(normal_df['depth'], 0.5)
            normal_df['new_alt'] = normal_df['depth'] - normal_df['new_ref']
            normal_df = normal_df.rename({'new_ref': 'n_refcount', 'new_alt':'n_altcount'}, axis=1)
            normal_df = normal_df.drop(['depth', 't_refcount', 't_altcount'], axis=1)
            
        normal_df['chr'] = mut.convert_chr(normal_df['chr'])
        normal_df['normal_totalcount'] = normal_df['n_refcount'] + normal_df['n_altcount']
        normal_df = normal_df.loc[normal_df['normal_totalcount'] > 0]

    elif filtered_variants_path is not None:
        # generate fake normal using binomial with N=30
        normal_df = pd.read_csv(filtered_variants_path, sep='\t')
        normal_df['n_altcount'] = s.binom.rvs(30, normal_df['t_altcount'] / normal_df['total_reads'])
        normal_df['n_refcount'] = 30 - normal_df['n_altcount']
        normal_df = normal_df.drop(['t_refcount', 't_altcount'], axis=1)
        normal_df['chr'] = mut.convert_chr(normal_df['chr'])
        normal_df['normal_totalcount'] = 30
    else:
        raise ValueError("either a normal callstats file or a filtered variants path must be passed")

    merged = snv_df.merge(normal_df, on=['chr', 'pos'], how='inner').drop_duplicates()

    # use conservative ascat filter for sites covered by normal > 20 counts 
    merged = merged.loc[merged.normal_totalcount > 20]
    
    # calculate ascat input logR and BAF
    merged['tumorLogR'] = merged['adjusted_depth'] / merged['normal_totalcount']
    merged['tumorLogR'] = np.log2(merged['tumorLogR'] / merged['tumorLogR'].mean())
    # ascat passes around a normalLogR file but never actually defines these values (nor are they ever used)
    # we will make a dummy file 
    merged['normalLogR'] = 0

    merged['normalBAF'] = np.nan
    merged['tumorBAF'] = np.nan
    # for unexplained reasons ascat also randomizes A and B alleles
    mask = np.random.rand(len(merged)) > 0.5
    merged.loc[mask, 'normalBAF'] = merged.loc[mask, 'n_refcount'] / merged.loc[mask, 'normal_totalcount']
    merged.loc[~mask, 'normalBAF'] = merged.loc[~mask, 'n_altcount'] / merged.loc[~mask, 'normal_totalcount']
    merged.loc[mask, 'tumorBAF'] = merged.loc[mask, 't_refcount'] / merged.loc[mask, 'adjusted_depth']
    merged.loc[~mask, 'tumorBAF'] = merged.loc[~mask, 't_altcount'] / merged.loc[~mask, 'adjusted_depth']

    # switch back to string X
    merged['chr'] = merged['chr'].apply(lambda x: 'X' if x ==23 else str(x))
    merged = merged.rename({'chr':'chrs'}, axis=1)
    # ascat expects the index to be in chr_pos form
    merged = merged.set_index(merged.apply(lambda x: x.chrs + '_' + str(x.pos), axis = 1))
    
    sample_name = '{}_{}_ascat'.format(out_label, purity)
    # ascat returns 4 seperate files each with one column of data
    merged.rename({'tumorLogR':sample_name}, axis=1)[['chrs', 'pos', sample_name]].to_csv(os.path.join(out_dir, '{}_{}_ascat_tumor_LogR.txt'.format(out_label, purity)), sep='\t')
    merged.rename({'normalLogR':sample_name}, axis=1)[['chrs', 'pos', sample_name]].to_csv(os.path.join(out_dir, '{}_{}_ascat_normal_LogR.txt'.format(out_label, purity)), sep='\t')
    merged.rename({'tumorBAF':sample_name}, axis=1)[['chrs', 'pos', sample_name]].to_csv(os.path.join(out_dir, '{}_{}_ascat_tumor_BAF.txt'.format(out_label, purity)), sep='\t')
    merged.rename({'normalBAF':sample_name}, axis=1)[['chrs', 'pos', sample_name]].to_csv(os.path.join(out_dir, '{}_{}_ascat_normal_BAF.txt'.format(out_label, purity)), sep='\t')

def generate_hatchet_files(sim_profile_pickle=None,
                           purity = None,
                           out_label=None,
                           normal_vcf_path=None,
                           tumor_baf_path=None,
                           int_counts_sim_fn = None, # files from preprocessing
                           pos_counts_sim_fn = None,
                           snp_counts_sim_fn = None,
                           read_combined_fn = None,
                           out_dir = "./",
                           parallel=False):
  
    if not os.path.isdir(out_dir):
        os.mkdir(out_dir)
    
    print("creating dummy sample file", flush=True) 
    # create dummy sample file
    if not os.path.isdir( os.path.join(out_dir, "hatchet_data")):
        os.mkdir(os.path.join(out_dir, "hatchet_data"))
    output_samples_txt = os.path.join(out_dir, "hatchet_data", "samples.txt")
  
    if out_label is None:
        out_label = 'benchmarking_simulation'
    normal_sim_label = out_label + '_normal'
    tumor_sim_label = out_label + '_tumor_sim'
    with open(output_samples_txt, 'w') as f:
        f.write(f'{normal_sim_label}\n')
        f.write(f'{tumor_sim_label}\n')
 
    read_combined_df = pd.read_csv(read_combined_fn, sep='\t')
 
    default_profile = pd.read_pickle(sim_profile_pickle)   

    interval_df = pd.read_csv(int_counts_sim_fn, sep='\t', names = ['chrom', 'start', 'end', 'covcorr'])
    pos_df = pd.read_csv(pos_counts_sim_fn, sep='\t', names = ['chrom', 'start', 'end', 'covcorr'])
        
    int_counts_tr_fn = os.path.join(out_dir, f'{out_label}_interval_counts.transformed.txt')
    pos_counts_tr_fn = os.path.join(out_dir, f'{out_label}_position_counts.transformed.txt')
    snp_counts_tr_fn = os.path.join(out_dir, f'{out_label}_snp_counts.transformed.txt')

    print("simulating coverage profile", flush=True)
    # alter total counts based on profile, for intervals (between SNP thresholds) and at SNP thresholds
    interval_corr_df_transformed = default_profile.compute_coverage(interval_df, purity, do_parallel=parallel)
    position_corr_df_transformed = default_profile.compute_coverage(pos_df, purity, do_parallel=parallel)
    
    # save these transformed dfs for posterity
    interval_corr_df_transformed.to_csv(int_counts_tr_fn, sep = '\t')
    position_corr_df_transformed.to_csv(pos_counts_tr_fn, sep = '\t')
    
    # alter SNP counts based on profile in tumor.1bed
    default_profile.save_hets_file(snp_counts_tr_fn, normal_vcf_path, snp_counts_sim_fn, purity, do_parallel=parallel)
    
    # need to reset index to set new column series
    read_combined_df = read_combined_df.reset_index(drop=True)
    read_combined_df['t_int_reads'] = interval_corr_df_transformed['covcorr']
    read_combined_df['t_pos_reads'] = position_corr_df_transformed['covcorr']
    
    read_combined_df = read_combined_df[['contig', 'n_int_reads', 'n_pos_reads', 't_int_reads', 't_pos_reads']]
    
    
    print("converting back to hatchet format", flush=True)
    for contig in read_combined_df['contig'].unique():
        read_combined_df[read_combined_df['contig'] == contig].drop(columns='contig').to_csv(os.path.join(out_dir, f'hatchet_data/{contig}.total.gz'), sep='\t', header=False, index=False, compression='gzip')

    # Total Counts
    with open(os.path.join(out_dir, 'hatchet_data/total.tsv'), 'w') as f:
        f.write(f"{normal_sim_label}\t{read_combined_df['n_int_reads'].sum()}\n")
        f.write(f"{tumor_sim_label}\t{read_combined_df['t_int_reads'].sum()}\n")

    # SNP Counts
    snp_df_transformed = pd.read_csv(snp_counts_tr_fn, sep='\t', header=0, names=['contig', 'pos', 'ref_count', 'alt_count'])
    snp_counts_1bed = pd.read_csv(tumor_baf_path, sep='\t', header=None, names=['contig', 'pos', 'sample', 'ref_count', 'alt_count'])
    snp_counts_out_df = pd.concat([snp_counts_1bed.iloc[:, :3], snp_df_transformed['ref_count'], snp_df_transformed['alt_count']], axis =1)
    snp_counts_out_df = snp_counts_out_df.astype({'pos':int, 'ref_count':int, 'alt_count':int})
    snp_counts_out_df['sample'] = tumor_sim_label
    snp_counts_out_df.to_csv(os.path.join(out_dir, 'hatchet_data/tumor.1bed'), sep='\t', header=False, index=False)
            
if __name__ == "__main__":
    main()
